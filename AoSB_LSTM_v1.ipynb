{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"PkFhjp-IPTPQ","executionInfo":{"status":"ok","timestamp":1713468755853,"user_tz":240,"elapsed":12378,"user":{"displayName":"Jesse Koppel","userId":"04907548838401524027"}}},"outputs":[],"source":["import sklearn.preprocessing as sklp\n","import sklearn.model_selection as sklm\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset\n","\n","import numpy as np\n","import pandas as pd"]},{"cell_type":"markdown","source":["## LSTM Model: Based on 4701 Implementation"],"metadata":{"id":"p0G9O-VhPe_B"}},{"cell_type":"markdown","source":["### Dataframe Creation:\n","\n","In this section, import relevant data on NFL games, preprocessed to include only relevant features and drop NA values:"],"metadata":{"id":"pnJhkHt8QdLN"}},{"cell_type":"code","source":["nfl_df = None # TODO: replace when data importation / preprocessing logic is implemented!!!"],"metadata":{"id":"O6rr543gQbRQ","executionInfo":{"status":"ok","timestamp":1713468758731,"user_tz":240,"elapsed":187,"user":{"displayName":"Jesse Koppel","userId":"04907548838401524027"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["### Custom Time Series Dataset:\n","\n","In this section, define a custom time-series dataset object for input into our LSTM module"],"metadata":{"id":"mvILvqNDPqJN"}},{"cell_type":"code","source":["class TimeSeriesDataset(Dataset):\n","    def __init__(self, data, n_lags, forecast_horizon=1, feature_cols=None, label_col='Close'):\n","        self.n_lags = n_lags\n","        self.forecast_horizon = forecast_horizon\n","        self.label_col = label_col\n","\n","        #allow selection of features\n","        if feature_cols:\n","          data = data[feature_cols].values\n","\n","        #normalize features\n","        self.scaler = sklp.MinMaxScaler()\n","        data_scaled = self.scaler.fit_transform(data)\n","\n","        #create usable data from scaled df\n","        self.X, self.y = self.create_sequences(data_scaled)\n","\n","    def create_sequences(self, data):\n","        X, y = [], []\n","        for i in range(len(data) - self.n_lags - self.forecast_horizon + 1):\n","            X.append(data[i:i + self.n_lags])\n","            y.append(data[i + self.n_lags + self.forecast_horizon - 1][3])\n","        return np.array(X), np.array(y)\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        return torch.tensor(self.X[idx], dtype=torch.float), torch.tensor(self.y[idx], dtype=torch.float)\n"],"metadata":{"id":"Azrht43jPekD","executionInfo":{"status":"ok","timestamp":1713468760625,"user_tz":240,"elapsed":161,"user":{"displayName":"Jesse Koppel","userId":"04907548838401524027"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#params for dataloader\n","n_lags = 4 #look back 4 games\n","forecast_horizon = 1 #look ahead 1 game\n","\n","ts_dataset = TimeSeriesDataset(nfl_df, n_lags, forecast_horizon)"],"metadata":{"id":"MUHUX06EPrTB","executionInfo":{"status":"error","timestamp":1713468771559,"user_tz":240,"elapsed":159,"user":{"displayName":"Jesse Koppel","userId":"04907548838401524027"}},"outputId":"cef3dc27-06ee-4a99-e519-b926aeda6a61","colab":{"base_uri":"https://localhost:8080/","height":373}},"execution_count":5,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Expected 2D array, got scalar array instead:\narray=nan.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-1d08d64b4265>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mforecast_horizon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m#look ahead 1 game\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mts_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeSeriesDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnfl_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_lags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforecast_horizon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-3-ef897e3d0770>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, n_lags, forecast_horizon, feature_cols, label_col)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#normalize features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mdata_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m#create usable data from scaled df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    876\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mfirst_pass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_samples_seen_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m    467\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfirst_pass\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    892\u001b[0m             \u001b[0;31m# If input is scalar raise error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    895\u001b[0m                     \u001b[0;34m\"Expected 2D array, got scalar array instead:\\narray={}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Expected 2D array, got scalar array instead:\narray=nan.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."]}]},{"cell_type":"code","source":["class LSTM(nn.Module):\n","    def __init__(self, num_layers, input_size, hidden_size, seq_length, num_classes=2):\n","        \"\"\"\n","        Inputs:\n","        num_layers: Number of recurrent layers\n","        input_size: Number of features for input\n","        hidden_size: Number of features in hidden state\n","        **these could be wrong**\n","        seq_length: Length of sequences in a batch\n","        num_classes: Number of categories for labels\n","\n","        Outputs: none\n","        \"\"\"\n","        super(LSTM, self).__init__()\n","\n","        self.num_classes = num_classes\n","        self.num_layers = num_layers\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.seq_length = seq_length\n","\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","\n","        self.dense = nn.Sequential(\n","            nn.ReLU(),\n","            nn.Linear(num_layers * hidden_size, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 15) # One output node per \"expected points entry\" (15 including offense, defense, spt.)\n","        )\n","\n","    def forward(self, x):\n","      '''\n","      Inputs:\n","      x: input data\n","\n","      Outputs:\n","      out: output of forward pass\n","      '''\n","\n","      out, (hn, cn) = self.lstm(x)\n","\n","      hn = hn.transpose(0, 1).reshape(x.size(0), -1)\n","\n","      x = self.dense(hn)\n","\n","      return x"],"metadata":{"id":"DkpMlufXPwze"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training / Validation / Testing:"],"metadata":{"id":"sWqTlIRZP3wM"}},{"cell_type":"code","source":["def val(model, val_loader, criterion):\n","    \"\"\"\n","    Inputs:\n","    model (torch.nn.Module): The deep learning model to be trained.\n","    val_data_loader (torch.utils.data.DataLoader): DataLoader for the validation dataset.\n","    criterion (torch.nn.Module): Loss function to compute the training loss.\n","\n","    Outputs:\n","    Validation Loss\n","    \"\"\"\n","    val_running_loss = 0\n","    num_correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for i, (inputs, labels) in enumerate(val_loader, 0):\n","\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","\n","            val_running_loss += loss.item()\n","\n","\n","    return val_running_loss"],"metadata":{"id":"hlVhem_xP6rk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model, train_loader, val_loader, criterion, epochs, optimizer):\n","  \"\"\"\n","    Inputs:\n","    model (torch.nn.Module): The deep learning model to be trained.\n","    train_loader (torch.utils.data.DataLoader): DataLoader for the training dataset.\n","    val_loader (torch.utils.data.DataLoader): DataLoader for the validation dataset.\n","    criterion (torch.nn.Module): Loss function to compute the training loss.\n","    epochs: Number of epochs to train for.\n","    optimizer: The optimizer to use during training.\n","\n","    Outputs:\n","    Tuple of (train_loss_arr, val_loss_arr, val_acc_arr)\n","  \"\"\"\n","  train_loss_arr = []\n","  val_loss_arr = []\n","  running_loss = 0.0\n","\n","\n","  for epoch in range(epochs):\n","      running_loss = 0.0\n","      for i, (inputs, labels) in enumerate(train_loader, 0):\n","\n","        optimizer.zero_grad()\n","        preds = model(inputs)\n","        loss = criterion(preds, labels)\n","\n","        running_loss += loss.item()\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","      val_loss = val(model, val_loader, criterion)\n","      train_loss_arr.append(running_loss)\n","      val_loss_arr.append(val_loss)\n","\n","      print(\"epoch:\", epoch+1, \"training loss:\", running_loss, 'val loss:', round(val_loss, 3))\n","\n","  print(running_loss)\n","  print('Training finished.')\n","\n","  return train_loss_arr, val_loss_arr"],"metadata":{"id":"JCTBCYYuP8gK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_layers = 2\n","input_size = 300\n","hidden_size = 64\n","seq_length = 40\n","num_classes = 2\n","\n","batch_size = 16\n","\n","train_size = int(0.8 * len(ts_dataset))\n","val_size = int(0.1*len(ts_dataset))\n","test_size = len(ts_dataset) - train_size - val_size\n","train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(ts_dataset, [train_size, val_size, test_size])\n","\n","# you may change the learning rate and numbers of epochs run\n","learning_rate = 0.01\n","lstm_epochs = 10\n","\n","criterion = nn.MSELoss()\n","\n","# Initialize LSTM model\n","lstm_model = LSTM(num_layers, input_size, hidden_size, seq_length, num_classes)\n","\n","#Initialize optimizer\n","optimizer = optim.Adam(lstm_model.parameters(), lr=learning_rate)\n","\n","#run training\n","lstm_train_loss, lstm_val_loss = train(lstm_model, train_loader, val_loader, criterion, lstm_epochs, optimizer)"],"metadata":{"id":"jtRi-Y0BQCbd"},"execution_count":null,"outputs":[]}]}